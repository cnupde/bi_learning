


# Thinking1 机器学习中的监督学习、非监督学习、强化学习有何区别

监督学习是带标签的学习，如逻辑回归等

非监督学习是不带标签的学习，如knn聚类等

监督学习与非监督学习的初始训练数据都是静态的，强化学习的训练数据是动态的，
环境会根据强化学习中每一步的结果对其进行奖励/惩罚，并将此作为下一轮的训练数据进行训练

# Thinking2 什么是策略网络，价值网络，有何区别
策略网络会根据当前状态输出一组策略及其相应的概率

价值网络会根据当前状态输出当前状态的的价值，是一个标量


# Thinking3 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的
MCTS（Monte Carlo Tree Search）：
蒙特卡洛树搜索，结合了随机模拟的一般性和树搜索的准确性
MCTS是一个搜索算法，它采用的各种方法都是为了有效地减少搜索空间。在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树
MCTS的作用是通过模拟来进行预测输出结果，理论上可以用于以{state,action}为定义的任何领域
使用主要步骤：
1. 选择，从根节点开始，按一定策略，搜索到叶子节点
2. 扩展，对叶子节点扩展一个或多个合法的子节点
3. 模拟，对子节点采用随机的方式（这也是为什么称之为蒙特卡洛的原因）模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数

MCTS原理：
每个节点代表一个局面，A/B代表被访问B次，黑棋赢了A次
我们将不断重复一个过程：
1. 选择Select，从根节点往下走，每次都选一个“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续着法的节点，比如 3/3 节点
2. 扩展Expansion，给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”
3. 模拟Simluation，用快速走子策略（Rollout policy）走到底，得到一个胜负结果（Thinking：为什么不采用AlphaGo的策略价值网络走棋）
4. 回传Backup，把模拟的结果加到它的所有父节点上，假设模拟的结果是 0/1，就把0/1加到所有父节点上

# Thinking4 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑
可以通过初版的推荐系统为用户进行信息流推荐，
然后把用户的反馈作为强化学习中的环境反馈模块，对本次推荐的内容进行奖励/惩罚，
并以此进行强化学习，不断完善推荐系统。

在这个系统里，用户的反馈行为最为重要，比如视频观看次数，停留时间等。


# Thinking5 在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路

强化学习需要环境反馈，在自动驾驶中环境反馈的代价较大（如造成交通事故等）。
因此可以通过三维仿真模拟来虚拟一个训练环境，将红绿灯、行人、行车、路口等信息进行模拟，
并设置相应的反馈机制，以此来实现自动驾驶的强化学习。

# Thinking6  Action（五子棋）：棋盘大小 10 * 10，采用强化学习（策略价值网络），用AI训练五子棋AI，请说明都有哪些模块，不同模块的原理


1. 神经网络：
    * 策略网络：输出是一个落子的概率分布，能学习到一些随机策略
    * 价值网络：输出是一个可能获胜的数值，即“价值”，对于价值网络当前局面价值=对终局的估计

2. MCTS

    1. 蒙特卡洛树搜索，结合了随机模拟的一般性和树搜索的准确性
    2. MCTS是一个搜索算法，它采用的各种方法都是为了有效地减少搜索空间。在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树
    3. MCTS的作用是通过模拟来进行预测输出结果，理论上可以用于以{state,action}为定义的任何领域

    使用主要步骤：

    1. 选择，从根节点开始，按一定策略，搜索到叶子节点
    2. 扩展，对叶子节点扩展一个或多个合法的子节点
    3. 模拟，对子节点采用随机的方式（这也是为什么称之为蒙特卡洛的原因）模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数

3. self-play
    
    1. 落子概率也称为策略（policy）
    2. 有了落子概率，简单的方式是直接按照这个概率进行落子 =>这会导致神经网络原地踏步，因为Policy Value Network的训练数据是自我对弈（self-play）
    3. 仅仅自己学习自己是不会有改进的，需要有一个办法来利用值函数的信息来优化这个策略
    4. 在AlphaGo系列算法里面是使用蒙特卡洛树搜索（MCTS）来进行策略优化的
    5. MCTS的输出是根据值函数V得到的一个更优策略，它将被用于通过self－play来生成数据供深度神经网络学习
    6. MCTS是AlphaGo能够通过self－play不断变强的重要原因


