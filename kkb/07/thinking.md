


# 请简述基于蒙特卡洛的强化学习的原理？
蒙特卡洛从经验（experience）中去学习，这个经验包括样本序列的状态（state）、动作（action）和奖励（reward）。得到若干样本的经验后，通过平均所有样本的回报（return）来解决强化学习的任务。

蒙特卡洛也可以看作是一种广义的策略迭代过程，即先计算当前策略所对应的值函数，再利用值函数来改进当前策略，不断循环这两个步骤，从而得到最优值函数和最优策略。


MCTS原理：
1. Step1，选择Select，从根节点往下走，每次都选一个“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续着法的节点，比如 3/3 节点
2. Step2，扩展Expansion，给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”
3. Step3，模拟Simluation，用快速走子策略（Rollout policy）走到底，得到一个胜负结果（Thinking：为什么不采用AlphaGo的策略价值网络走棋）
4. Step4，回传Backup，把模拟的结果加到它的所有父节点上，假设模拟的结果是 0/1，就把0/1加到所有父节点上 

# 强化学习的目标与深度学习的目标有何区别？
深度学习的目标是端到端的具体任务学习，强化学习的目标是在不断地环境反馈（学习）中强化现有AI。

深度强化学习是强化学习结合了深度学习而延伸出的概念。强化学习有agent、environment、reward、action等组成部分，就是一个智能体（agent）在一个未知的环境（environment）中，不断摸索，将动作（action）作用于环境，环境反馈奖励（reward）给智能体，然后智能体根据奖励来更新这个产生动作的决策函数。当环境越来越复杂，这个决策函数进行决策和实现起来就越来越困难，而深度神经网络正好具有强大的拟合能力，所以可以将这个决策函数用深度神经网络来代替，这样就形成了深度强化学习。深度学习则主要是以神经网络增加隐层个数而形成深度神经网络来进行学习，它在学习时，学习的数据和环境都是已知的，所以只需学习如何去拟合函数就可以了。

